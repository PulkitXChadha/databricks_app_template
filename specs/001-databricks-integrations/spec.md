# Feature Specification: Databricks App Template with Service Integrations

**Feature Branch**: `001-databricks-integrations`  
**Created**: Saturday, October 4, 2025  
**Status**: Draft  
**Input**: User description: "I want to build a Databricks App template that showcases all of the integration points with Databricks services, such as Lakebase, the backend transactional database, serving models as from models, serving endpoints, and Databricks Asset Bundles for deployment."

## Execution Flow (main)
```
1. Parse user description from Input
   â†’ If empty: ERROR "No feature description provided"
2. Extract key concepts from description
   â†’ Identify: actors, actions, data, constraints
3. For each unclear aspect:
   â†’ Mark with [NEEDS CLARIFICATION: specific question]
4. Fill User Scenarios & Testing section
   â†’ If no clear user flow: ERROR "Cannot determine user scenarios"
5. Generate Functional Requirements
   â†’ Each requirement must be testable
   â†’ Mark ambiguous requirements
6. Identify Key Entities (if data involved)
7. Run Review Checklist
   â†’ If any [NEEDS CLARIFICATION]: WARN "Spec has uncertainties"
   â†’ If implementation details found: ERROR "Remove tech details"
8. Return: SUCCESS (spec ready for planning)
```

---

## âš¡ Quick Guidelines
- âœ… Focus on WHAT users need and WHY
- âŒ Avoid HOW to implement (no tech stack, APIs, code structure)
- ðŸ‘¥ Written for business stakeholders, not developers

### Section Requirements
- **Mandatory sections**: Must be completed for every feature
- **Optional sections**: Include only when relevant to the feature
- When a section doesn't apply, remove it entirely (don't leave as "N/A")
- **âš ï¸ IMPORTANT**: Constitutional principles (`.specify/memory/constitution.md`) are ALWAYS mandatory regardless of feature scope. The "optional sections" guideline applies only to spec template sections like "User Stories" or "Edge Cases", NOT to constitutional requirements (Design Bricks, Lakebase, Asset Bundles, Type Safety, Model Serving, Auto-Generated Clients, Development Tooling, Observability, Multi-User Isolation).

### For AI Generation
When creating this spec from a user prompt:
1. **Mark all ambiguities**: Use [NEEDS CLARIFICATION: specific question] for any assumption you'd need to make
2. **Don't guess**: If the prompt doesn't specify something (e.g., "login system" without auth method), mark it
3. **Think like a tester**: Every vague requirement should fail the "testable and unambiguous" checklist item
4. **Common underspecified areas**:
   - User types and permissions
   - Data retention/deletion policies  
   - Performance targets and scale
   - Error handling behaviors
   - Integration requirements
   - Security/compliance needs

---

## Technical Prerequisites *(mandatory)*

### Runtime Environment
- **Python Version**: 3.11 or higher (required for modern type hints and performance optimizations)
- **Node.js Version**: 18.0 or higher (required for TypeScript 5.2+ and Vite 5.0)
- **Databricks SDK**: v0.56.0 or higher (required for Lakebase OAuth token generation via `generate_database_credential()` API)
- **Databricks Runtime**: Compatible with Databricks Apps platform (serverless compute)
- **Browser Support**: Modern browsers with ES2020+ support (Chrome 90+, Firefox 88+, Safari 14+, Edge 90+)

### Platform Requirements
- Active Databricks workspace with Unity Catalog enabled
- Lakebase (Databricks-hosted Postgres) provisioned and accessible
- Model Serving endpoint creation permissions
- Asset Bundles deployment permissions

### Terminology Conventions
This specification uses consistent naming conventions across different contexts:
- **Specification Prose**: Multi-word entity names with spaces (e.g., "User Session", "Data Source")
- **Python Classes**: PascalCase without spaces (e.g., `UserSession`, `DataSource`)
- **SQL Tables**: snake_case (e.g., `user_preferences`, `model_inference_logs`)
- **JSON Fields**: snake_case for API contracts (e.g., `user_id`, `preference_key`)
- **TypeScript Interfaces**: PascalCase matching Python models (e.g., `UserSession`, `DataSource`)

### UI Framework Note
The implementation uses the `designbricks` package (v0.2.2) for Databricks-styled UI components (TopBar, Sidebar). Static content pages use shadcn/ui for enhanced developer experience. This hybrid approach balances Databricks branding (via designbricks navigation components) with development velocity (via shadcn/ui for forms and content).

---

## Clarifications

### Session 2025-10-04
- Q: Which lakehouse storage approach should the template demonstrate? â†’ A: Unity Catalog managed tables with fine-grained access control
- Q: Which model registry should the template demonstrate for serving models? â†’ A: Unity Catalog model registry (modern, governance-focused)
- Q: Which transactional database should the template use for backend data persistence? â†’ A: Lakebase (Databricks-hosted Postgres for transactional workloads)
- Q: Which authentication method(s) should the template support for Databricks service connections? â†’ A: Databricks Apps built-in authentication (app service principal + user authorization)
- Q: What authentication mechanism should be used for Lakebase database connections? â†’ A: OAuth token authentication exclusively using Databricks SDK's `workspace_client.database.generate_database_credential()` API (introduced in SDK v0.56.0). Tokens expire after 1 hour but open connections remain active. The application should generate fresh tokens for each new connection attempt. PAT (Personal Access Token) authentication is not supported. **IMPORTANT**: The instance name parameter must use the logical bundle name (e.g., `databricks-app-lakebase-dev` from databricks.yml), NOT the technical UUID extracted from the host (e.g., `instance-0fac1568-...`). Set via `LAKEBASE_INSTANCE_NAME` environment variable.
- Q: What level of customization should the template support for developers? â†’ A: Both - provide config-based customization with clear extension points for code changes
- Q: Should the template include automated setup scripts that create sample data for demonstration purposes? â†’ A: Hybrid - include minimal sample data creation with clear instructions to connect real resources
- Q: Should the template demonstrate observability and monitoring capabilities? â†’ A: Yes - include logging, metrics, and tracing examples using Databricks observability tools
- Q: What type of user interface should the template application provide? â†’ A: Web-based UI (interactive dashboard or web application)
- Q: Should the template demonstrate multi-user concurrent access patterns? â†’ A: Multi-user with data isolation - demonstrate user-specific data views and state
- Q: What is the primary performance/scale target for this template application? â†’ A: Balanced - demonstrate scalable patterns while keeping code readable for learning
- Q: What should be explicitly OUT OF SCOPE for this template? â†’ A: Advanced production features (auto-scaling, disaster recovery, multi-region), data ingestion/ETL workflows, and custom authentication beyond Databricks Apps built-in auth
- Q: How long should application data be retained? â†’ A: Indefinite - no automatic cleanup (developer manually manages data lifecycle)
- Q: Should the template actively support multi-environment deployment? â†’ A: Dev + Prod only - support two environments (development and production) with tested configurations
- Q: What level of error tracking should be implemented? â†’ A: Basic logging only - write errors to structured logs (console/file) with timestamp and context
- Q: What level of dashboard interactivity should be included? â†’ A: Full CRUD operations - create/update/delete records in Lakebase, modify user preferences, manage configurations
- Q: When two users simultaneously update the same Lakebase record (e.g., a shared configuration), how should the application handle the conflict? â†’ A: Not applicable - all Lakebase records are user-isolated (no shared records)
- Q: How should the application behave when a user exceeds reasonable API request limits (e.g., rapidly clicking "Invoke Model" or refreshing data)? â†’ A: No rate limiting - allow all requests (acceptable for demo/template purposes)
- Q: What minimum accessibility (a11y) standard should the web UI meet? â†’ A: WCAG 2.1 Level A - minimal compliance (keyboard navigation, alt text for images)
- Q: Should Unity Catalog table queries support pagination, or is returning all results acceptable for the demo/template scope? â†’ A: Basic pagination - support limit/offset parameters in API, demonstrate pattern in UI

---

## Out of Scope *(mandatory)*

The following features and capabilities are explicitly excluded from this template to maintain focus on core service integration demonstrations:

- **Advanced Production Operations**: Auto-scaling configurations, disaster recovery procedures, multi-region deployment strategies, advanced high-availability patterns
- **Data Ingestion & ETL Workflows**: Data pipeline creation, scheduled data imports, streaming data ingestion, complex transformation logic, data quality checks, or orchestration workflows (template focuses only on querying existing data in Unity Catalog)
- **Custom Authentication Systems**: Custom user authentication mechanisms, OAuth provider integration, SAML/SSO configuration, custom authorization logic beyond Databricks Apps built-in authentication (service principal and on-behalf-of-user patterns)
- **Production-Grade Security Hardening**: Network security configurations, VPC/firewall rules, secrets rotation automation, compliance certifications, rate limiting/throttling mechanisms (template demonstrates secure integration patterns but not full production hardening)
- **Advanced Model Management**: Model training workflows, hyperparameter tuning, model versioning automation, A/B testing infrastructure, model monitoring/drift detection (template focuses on serving pre-existing models)
- **Advanced Error Tracking & Monitoring**: External APM tools (Datadog, New Relic, Sentry), automated alerting systems, custom error dashboards, error rate SLOs/SLAs (template uses basic structured logging with Databricks observability tools)

---

## User Scenarios & Testing *(mandatory)*

### Primary User Story
As a developer evaluating Databricks for application development, I need a comprehensive template web application that demonstrates how to integrate with core Databricks services including data storage, model serving, and deployment automation through an interactive dashboard with well-documented, scalable code patterns, so that I can understand best practices and quickly bootstrap my own production-ready applications.

### Acceptance Scenarios
1. **Given** a developer has access to a Databricks workspace, **When** they deploy the template application and access the web UI, **Then** they see an interactive dashboard displaying data from Unity Catalog tables
2. **Given** the template application is running, **When** a user creates, reads, updates, or deletes records (user preferences, configurations, application state), **Then** the application successfully performs CRUD operations in Lakebase (Databricks-hosted Postgres) and displays confirmation of each operation
3. **Given** machine learning models are deployed, **When** the application needs predictions or model outputs, **Then** it successfully invokes models through serving endpoints and displays results
4. **Given** the template code is modified, **When** a developer triggers deployment to either development or production environment, **Then** the application is packaged and deployed using Databricks Asset Bundles with environment-specific configurations (dev uses development workspace resources, prod uses production workspace resources)
5. **Given** a new developer clones the template, **When** they run the provided setup scripts, **Then** minimal sample data is created and the application successfully demonstrates all service integrations
6. **Given** a developer wants to use their own Databricks resources, **When** they follow the configuration documentation, **Then** they can replace sample data connections with their existing Unity Catalog tables, Lakebase databases, and model endpoints
7. **Given** the application is running, **When** service operations execute (data queries, model invocations, authentication), **Then** structured logs, metrics, and traces are emitted to Databricks observability tools for monitoring and debugging
8. **Given** multiple users access the application concurrently, **When** each user queries data or saves preferences, **Then** each user sees only data they have permission to access via Unity Catalog and their personal state is isolated in Lakebase
9. **Given** a developer reviews the template code, **When** they examine integration implementations, **Then** they find well-commented code demonstrating scalable patterns (connection pooling, efficient queries) balanced with readability for learning purposes

### Edge Cases & Error Handling Requirements
- **EC-001 (Model Serving Unavailable)**: When a Model Serving endpoint is unavailable (connection timeout >30s) or returns 5xx error, the application MUST return HTTP 503 with JSON error response: `{"error_code": "MODEL_UNAVAILABLE", "message": "Model service temporarily unavailable. Please try again in a few moments.", "technical_details": {"endpoint": "<url>", "status": <code>}, "retry_after": 30}`. The error MUST be logged with ERROR level including request context.
- **EC-002 (Lakebase Connection Failure)**: When Lakebase connection fails (network error, authentication failure, OAuth token generation failure, or connection pool exhausted), the application MUST return HTTP 503 with JSON error response: `{"error_code": "DATABASE_UNAVAILABLE", "message": "Database service temporarily unavailable.", "technical_details": {"error_type": "<postgres_error_code|oauth_error>", "instance_name": "<instance_id>"}, "retry_after": 10}`. Failed connections MUST trigger connection pool health check and emit metric. OAuth token generation failures should be logged with full error context for debugging.
- **EC-003 (Missing Credentials/Permissions)**: When Databricks credentials are missing, invalid, or lack required permissions, the application MUST return HTTP 401 (missing/invalid credentials) or HTTP 403 (insufficient permissions) with JSON error response: `{"error_code": "AUTH_REQUIRED|PERMISSION_DENIED", "message": "<user-friendly description>", "technical_details": {"required_scope": "<scope>", "workspace": "<workspace_url>"}}`. The UI MUST display credential setup instructions from documentation.
- **EC-004 (Empty/Inaccessible Unity Catalog Tables)**: When Unity Catalog tables are empty, the application MUST return HTTP 200 with empty results array and metadata indicating zero rows. When tables are inaccessible due to permissions, return HTTP 403 with JSON error: `{"error_code": "CATALOG_PERMISSION_DENIED", "message": "You don't have access to this table.", "technical_details": {"catalog": "<name>", "schema": "<name>", "table": "<name>"}}`. When tables don't exist, return HTTP 404.
- **EC-005 (Invalid Asset Bundle Configuration)**: When Asset Bundle configuration is invalid (schema errors, missing required fields, invalid target references), the `databricks bundle validate` command MUST fail with exit code 1 and display specific validation errors. The application MUST NOT deploy with invalid configuration. Documentation MUST include common validation error resolutions.

## Requirements *(mandatory)*

### Functional Requirements
- **FR-001**: Application MUST demonstrate reading data from Unity Catalog managed tables with fine-grained access control
- **FR-002**: Application MUST demonstrate full CRUD operations (Create, Read, Update, Delete) on data in Lakebase (Databricks-hosted Postgres) as the transactional database backend, including user preferences, configurations, and application state
- **FR-003**: Application MUST demonstrate invoking machine learning models through Model Serving endpoints and displaying results
- **FR-004**: Application MUST demonstrate serving models from Unity Catalog model registry via Model Serving endpoints. Model inference (invoke_model) from UC-registered models is the mandatory capability. Metadata retrieval operations (list_endpoints, get_endpoint) SHOULD be implemented for UI display and debugging purposes and are considered standard practice for complete integration demonstration
- **FR-005**: Application MUST be deployable using Databricks Asset Bundles to both development and production environments with tested, environment-specific configurations
- **FR-006**: Application MUST provide clear documentation for each service integration demonstrated: README section describing all integrations, OpenAPI specs for all endpoints (auto-generated by FastAPI and accessible at `/docs` and `/openapi.json` endpoints for validation), inline code comments (â‰¥1 docstring per public function/method, where "public" means module-level, non-underscore-prefixed functions/methods or those documented in module `__all__`; excludes nested functions, lambdas, and private helpers), and service setup guide
- **FR-007**: Application MUST handle and display appropriate error messages when service integrations fail (JSON format with error_code, user-friendly message, and technical details for debugging)
- **FR-008**: Application MUST include working configurations for connecting to each service: .env.local.example with all required environment variables, databricks.yml with functional dev and prod target configurations (tested and ready to use), and sample data configuration documentation
- **FR-009**: Application MUST authenticate securely with Databricks services using Databricks Apps built-in authentication with three distinct patterns: (a) Service Principal authentication (app-level authorization) for shared/system operations (e.g., health checks, system-level queries), (b) On-Behalf-Of-User authentication (user-level authorization) for user-specific data access (e.g., querying Unity Catalog with user's permissions, saving user preferences), and (c) Lakebase OAuth token authentication exclusively using `workspace_client.database.generate_database_credential()` API for database connections. PAT (Personal Access Token) authentication is not supported. Documentation MUST include clear examples of when to use each pattern.
- **FR-010**: Developer users MUST be able to customize the template through configuration files (environment variables, YAML configs) with clear extension points for code modifications when deeper customization is needed
- **FR-011**: Application MUST provide a web-based interactive dashboard with full CRUD operations including: (a) viewing data from Unity Catalog with basic pagination (limit/offset parameters in API, page navigation controls in UI), optional filtering/sorting, (b) creating, reading, updating, and deleting records in Lakebase (user preferences, configurations, application state), (c) invoking model predictions with custom inputs, (d) visual feedback showing successful integration with each service (authentication status, connection health, operation results)
- **FR-012**: Application MUST include minimal sample data creation scripts for quick demonstration (â‰¤1 Unity Catalog catalog, â‰¤2 schemas, â‰¤3 tables with â‰¤100 rows each, â‰¤5 Lakebase sample records), along with clear documentation for connecting to existing Databricks resources (Unity Catalog tables, Lakebase databases, Model Serving endpoints). Note: Model Serving endpoint setup requires pre-existing endpoint; see `docs/databricks_apis/model_serving_setup.md` for endpoint creation instructions via Databricks UI or CLI (sample data script does not automate endpoint creation)
- **FR-013**: Application MUST demonstrate observability best practices including structured logging (JSON format with timestamp, log level, context fields, error details), basic application metrics examples, and correlation-ID based request tracking (simplified distributed tracing using contextvars, not full OpenTelemetry; see research.md for implementation pattern) using Databricks observability tools. All errors MUST be logged with ERROR level including timestamp, error type, error message, request context, and relevant technical details for debugging
- **FR-014**: Application MUST demonstrate multi-user access with data isolation, where each user sees data filtered according to their Unity Catalog permissions and has separate user-specific state in Lakebase
- **FR-015**: Application code MUST balance production-ready scalability patterns (efficient queries, appropriate error handling, connection pooling) with code clarity and inline documentation to serve as a learning resource

### Non-Functional Requirements
- **NFR-001**: Application MUST maintain readable, well-commented code that explains integration patterns and design decisions with measurable criteria: â‰¥1 docstring per public function/method, â‰¥80% of functions with type hints (measured as: count of module-level functions with return type annotation Ã· total module-level functions â‰¥ 0.80; verified via mypy --strict coverage report), cyclomatic complexity score â‰¤10 per function (measured by ruff/radon), inline comments for non-obvious logic (â‰¥1 comment per 20 lines for functions with cyclomatic complexity >5)
- **NFR-002**: Application MUST demonstrate scalable data access patterns (connection pooling for â‰¥10 connections, efficient query patterns) without over-optimizing at the expense of code clarity
- **NFR-003**: Application SHOULD handle typical development/demo workloads efficiently with baseline single-user performance targets (<500ms API response time for paginated Unity Catalog queries with limit â‰¤100 rows per page, <2s for model inference with standard input payload) and maintain acceptable performance under load (support 10 concurrent users with uniform request distribution resulting in <20% latency increase compared to single-user baseline)
- **NFR-004**: Web UI MUST meet WCAG 2.1 Level A accessibility standards including: all interactive elements accessible via keyboard navigation (Tab, Enter, Escape keys), all non-text content has text alternatives (alt text for images, aria-label for icon buttons), form inputs have associated labels, sufficient color contrast for text (minimum 3:1 for large text â‰¥18pt, 4.5:1 for normal text)

### Key Entities *(include if feature involves data)*
- **User Workspace**: Represents a developer's Databricks workspace containing resources and configurations needed to run the application
- **Data Source**: Represents Unity Catalog managed tables with associated fine-grained access controls that the application reads from
- **Transactional Record**: Represents data stored in Lakebase (Databricks-hosted Postgres) for transactional workloads (e.g., user preferences, application state, audit logs). All records are strictly user-isolated (no shared records exist; each record belongs to exactly one user). Data retention policy: indefinite (no automatic cleanup; developers manually manage data lifecycle). Uniqueness constraint: each record type uses user-scoped primary keys (e.g., composite key of user_id + preference_key for user preferences)
- **User Session**: Represents an authenticated user's interaction session with the web application, including their identity and permissions context
- **Model**: Represents a machine learning model registered in Unity Catalog model registry and deployed to a serving endpoint that can accept requests and return predictions
- **Model Serving Endpoint**: Represents a model deployment that exposes model inference capabilities via API
- **Deployment Configuration**: Represents the Asset Bundle configuration defining how the application is packaged and deployed
- **Service Integration**: Represents a connection point between the application and a Databricks service, including connection details and status

---

## Review & Acceptance Checklist
*GATE: Automated checks run during main() execution*

### Content Quality
- [ ] No implementation details (languages, frameworks, APIs)
- [ ] Focused on user value and business needs
- [ ] Written for non-technical stakeholders
- [ ] All mandatory sections completed

### Requirement Completeness
- [ ] No [NEEDS CLARIFICATION] markers remain
- [ ] Requirements are testable and unambiguous  
- [ ] Success criteria are measurable
- [ ] Scope is clearly bounded
- [ ] Dependencies and assumptions identified

---

## Execution Status
*Updated by main() during processing*

- [x] User description parsed
- [x] Key concepts extracted
- [x] Ambiguities marked
- [x] User scenarios defined
- [x] Requirements generated
- [x] Entities identified
- [x] Review checklist passed (All clarifications resolved)
- [x] Implementation in progress (35/49 tasks complete - 71%)

## Implementation Status (October 7, 2025)

**Core Implementation**: âœ… COMPLETE (71% of tasks done)
- Backend services fully implemented (Unity Catalog, Lakebase, Model Serving)
- Frontend fully functional with DatabricksServicesPage integrating all three services
- Database migrations complete (user_preferences, model_inference_logs)
- TypeScript client auto-generated from OpenAPI spec
- Observability infrastructure with structured logging and correlation IDs
- All CRUD operations working (Unity Catalog queries, Preferences management, Model inference)

**Architecture**:
- Main UI: DatabricksServicesPage with `designbricks` TopBar/Sidebar navigation
- Tabbed interface: Welcome, Unity Catalog, Model Serving, Preferences
- API integration: UnityCatalogService, LakebaseService, ModelServingService (TypeScript)
- Components: DataTable (pagination), PreferencesForm (JSON editor), ModelInvokeForm (endpoint selector)

**Remaining Work**:
- Integration testing (multi-user isolation, observability, accessibility, performance)
- Asset Bundle validation
- End-to-end validation and deployment testing
- Documentation updates (CLAUDE.md)

---
*Aligned with Constitution v1.0.0 - See `.specify/memory/constitution.md`*
